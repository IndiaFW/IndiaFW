{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "visionaries_project.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IndiaFW/IndiaFW/blob/main/visionaries_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0icTBkdDdljQ"
      },
      "source": [
        "# Simulating Semantic Dementia\n",
        "The goal of our project is to better understand the modelling approach and tools of Neuromatch Academy through simulation. \n",
        "\n",
        "We will simulate Semantic Dementia, a neurocognitive disorder that is characterised by loss of verbal, semantic discriminability. In our study, we reduce the dimensionality of information available for decoding and encoding models and examine how this loss of information affects classification accuracy. This is an extremely simplified illustration of semantic dementia but provides a foundation to use the tools we are learning.\n",
        "\n",
        "We use the Kay natural image dataset for this simulation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctAfkLyzNZXm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgZXo_bJ7NJT"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import minimize\n",
        "from scipy.io import loadmat\n",
        "\n",
        "#@title Preliminaries\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\n",
        "\n",
        "def plot_stim_and_spikes(stim, spikes, dt, nt=120):\n",
        "  \"\"\"Show time series of stim intensity and spike counts.\n",
        "\n",
        "  Args:\n",
        "    stim (1D array): vector of stimulus intensities\n",
        "    spikes (1D array): vector of spike counts\n",
        "    dt (number): duration of each time step\n",
        "    nt (number): number of time steps to plot\n",
        "\n",
        "  \"\"\"\n",
        "  timepoints = np.arange(120)\n",
        "  time = timepoints * dt\n",
        "\n",
        "  f, (ax_stim, ax_spikes) = plt.subplots(\n",
        "    nrows=2, sharex=True, figsize=(8, 5),\n",
        "  )\n",
        "  ax_stim.plot(time, stim[timepoints])\n",
        "  ax_stim.set_ylabel('Stimulus intensity')\n",
        "\n",
        "  ax_spikes.plot(time, spikes[timepoints])\n",
        "  ax_spikes.set_xlabel('Time (s)')\n",
        "  ax_spikes.set_ylabel('Number of spikes')\n",
        "\n",
        "  f.tight_layout()\n",
        "\n",
        "\n",
        "def plot_glm_matrices(X, y, nt=50):\n",
        "  \"\"\"Show X and Y as heatmaps.\n",
        "\n",
        "  Args:\n",
        "    X (2D array): Design matrix.\n",
        "    y (1D or 2D array): Target vector.\n",
        "\n",
        "  \"\"\"\n",
        "  from matplotlib.colors import BoundaryNorm\n",
        "  from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "  Y = np.c_[y]  # Ensure Y is 2D and skinny\n",
        "\n",
        "  f, (ax_x, ax_y) = plt.subplots(\n",
        "    ncols=2,\n",
        "    figsize=(6, 8),\n",
        "    sharey=True,\n",
        "    gridspec_kw=dict(width_ratios=(5, 1)),\n",
        "  )\n",
        "  norm = BoundaryNorm([-1, -.2, .2, 1], 256)\n",
        "  imx = ax_x.pcolormesh(X[:nt], cmap=\"coolwarm\", norm=norm)\n",
        "\n",
        "  ax_x.set(\n",
        "    title=\"X\\n(lagged stimulus)\",\n",
        "    xlabel=\"Time lag (time bins)\",\n",
        "    xticks=[4, 14, 24],\n",
        "    xticklabels=['-20', '-10', '0'],\n",
        "    ylabel=\"Time point (time bins)\",\n",
        "  )\n",
        "  plt.setp(ax_x.spines.values(), visible=True)\n",
        "\n",
        "  divx = make_axes_locatable(ax_x)\n",
        "  caxx = divx.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
        "  cbarx = f.colorbar(imx, cax=caxx)\n",
        "  cbarx.set_ticks([-.6, 0, .6])\n",
        "  cbarx.set_ticklabels(np.sort(np.unique(X)))\n",
        "\n",
        "  norm = BoundaryNorm(np.arange(y.max() + 1), 256)\n",
        "  imy = ax_y.pcolormesh(Y[:nt], cmap=\"magma\", norm=norm)\n",
        "  ax_y.set(\n",
        "    title=\"Y\\n(spike count)\",\n",
        "    xticks=[]\n",
        "  )\n",
        "  ax_y.invert_yaxis()\n",
        "  plt.setp(ax_y.spines.values(), visible=True)\n",
        "\n",
        "  divy = make_axes_locatable(ax_y)\n",
        "  caxy = divy.append_axes(\"right\", size=\"30%\", pad=0.1)\n",
        "  cbary = f.colorbar(imy, cax=caxy)\n",
        "  cbary.set_ticks(np.arange(y.max()) + .5)\n",
        "  cbary.set_ticklabels(np.arange(y.max()))\n",
        "\n",
        "def plot_spike_filter(theta, dt, **kws):\n",
        "  \"\"\"Plot estimated weights based on time lag model.\n",
        "\n",
        "  Args:\n",
        "    theta (1D array): Filter weights, not including DC term.\n",
        "    dt (number): Duration of each time bin.\n",
        "    kws: Pass additional keyword arguments to plot()\n",
        "\n",
        "  \"\"\"\n",
        "  d = len(theta)\n",
        "  t = np.arange(-d + 1, 1) * dt\n",
        "\n",
        "  ax = plt.gca()\n",
        "  ax.plot(t, theta, marker=\"o\", **kws)\n",
        "  ax.axhline(0, color=\".2\", linestyle=\"--\", zorder=1)\n",
        "  ax.set(\n",
        "    xlabel=\"Time before spike (s)\",\n",
        "    ylabel=\"Filter weight\",\n",
        "  )\n",
        "\n",
        "def compute_accuracy(X, y, model):\n",
        "  \"\"\"Compute accuracy of classifier predictions.\n",
        "  Args:\n",
        "    X (2D array): Data matrix\n",
        "    y (1D array): Label vector\n",
        "    model (sklearn estimator): Classifier with trained weights.\n",
        "  Returns:\n",
        "    accuracy (float): Proportion of correct predictions.\n",
        "  \"\"\"\n",
        "\n",
        "  y_pred = model.predict(X)\n",
        "\n",
        "  accuracy = (y == y_pred).mean()\n",
        "\n",
        "  return accuracy\n",
        "\n",
        "def plot_spikes_with_prediction(\n",
        "    spikes, predicted_spikes, dt, nt=50, t0=120, **kws):\n",
        "  \"\"\"Plot actual and predicted spike counts.\n",
        "\n",
        "  Args:\n",
        "    spikes (1D array): Vector of actual spike counts\n",
        "    predicted_spikes (1D array): Vector of predicted spike counts\n",
        "    dt (number): Duration of each time bin.\n",
        "    nt (number): Number of time bins to plot\n",
        "    t0 (number): Index of first time bin to plot.\n",
        "    kws: Pass additional keyword arguments to plot()\n",
        "\n",
        "  \"\"\"\n",
        "  t = np.arange(t0, t0 + nt) * dt\n",
        "\n",
        "  f, ax = plt.subplots()\n",
        "  lines = ax.stem(t, spikes[:nt], use_line_collection=True)\n",
        "  plt.setp(lines, color=\".5\")\n",
        "  lines[-1].set_zorder(1)\n",
        "  kws.setdefault(\"linewidth\", 3)\n",
        "  yhat, = ax.plot(t, predicted_spikes[:nt], **kws)\n",
        "  ax.set(\n",
        "      xlabel=\"Time (s)\",\n",
        "      ylabel=\"Spikes\",\n",
        "  )\n",
        "  ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
        "  ax.legend([lines[0], yhat], [\"Spikes\", \"Predicted\"])\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def plot_accuracy(the_data):\n",
        "  \"\"\"Plot accuracy of cross validation.\n",
        "\n",
        "  Args:\n",
        "    the_data (1D array): vector of crossvalidation accuracies\n",
        "    \n",
        "  \"\"\"\n",
        "  f, ax = plt.subplots(figsize=(8, 3))\n",
        "  ax.boxplot(the_data, vert=False, widths=.7)\n",
        "  ax.scatter(the_data, np.ones(8))\n",
        "  ax.set(\n",
        "    xlabel=\"Accuracy\",\n",
        "    yticks=[],\n",
        "    title=f\"Cross-validation accuracy: {accuracies.mean():.2%}\"\n",
        "  )\n",
        "  ax.spines[\"left\"].set_visible(False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prggAdvg7NJX",
        "cellView": "form"
      },
      "source": [
        "#@title Load data\n",
        "fname = \"kay_labels.npy\"\n",
        "if not os.path.exists(fname):\n",
        "  !wget -qO $fname https://osf.io/r638s/download\n",
        "fname = \"kay_labels_val.npy\"\n",
        "if not os.path.exists(fname):\n",
        "  !wget -qO $fname https://osf.io/yqb3e/download\n",
        "fname = \"kay_images.npz\"\n",
        "if not os.path.exists(fname):\n",
        "  !wget -qO $fname https://osf.io/ymnjv/download"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jotSNElmJNx"
      },
      "source": [
        "# Simple Linear Regression approach\n",
        "Let's try to predict neural response from image data. Project TA recommended we start with the standard decoding approach (predict image identity from neural data) and then reverse things. \n",
        "\n",
        "Here's the plan of attack recommended by Andrew:\n",
        "\n",
        "1. PCA on the voxels\n",
        "1. Train LDA on the top principle components to predict the image labels\n",
        "1. Evaulate model fit\n",
        "1. Cut out some of the weights (simulating a \"loss of information\" in the clinical condition)\n",
        "1. Attempt evaluation in reverse (predict voxel PCAs from image data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x7gGowzaQ0z",
        "cellView": "form"
      },
      "source": [
        "#@title Load PCA helper functions\n",
        "\n",
        "def plot_KAY_sample(X):\n",
        "  \"\"\"\n",
        "  Plots 9 images in the Kay natural image dataset.\n",
        "\n",
        "  Args:\n",
        "     X (numpy array of floats) : Data matrix each column corresponds to a\n",
        "                                 different image\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  k = 0\n",
        "  for k1 in range(3):\n",
        "    for k2 in range(3):\n",
        "      k = k + 1\n",
        "      plt.imshow(np.reshape(X[k,:],[128,128]),\n",
        "                 extent=[(k1 + 1) * 128, k1 * 128, (k2+1) * 128, k2 * 128])\n",
        "\n",
        "  plt.xlim((3 * 128, 0))\n",
        "  plt.ylim((3 * 128, 0))\n",
        "  plt.tick_params(axis='both', which='both', bottom=False, top=False,\n",
        "                  labelbottom=False)\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "  plt.show()\n",
        "\n",
        "def change_of_basis(X, W):\n",
        "  \"\"\"\n",
        "  Projects data onto a new basis.\n",
        "\n",
        "  Args:\n",
        "    X (numpy array of floats) : Data matrix each column corresponding to a\n",
        "                                different random variable\n",
        "    W (numpy array of floats) : new orthonormal basis columns correspond to\n",
        "                                basis vectors\n",
        "\n",
        "  Returns:\n",
        "    (numpy array of floats)   : Data matrix expressed in new basis\n",
        "  \"\"\"\n",
        "\n",
        "  Y = np.matmul(X, W)\n",
        "\n",
        "  return Y\n",
        "\n",
        "\n",
        "def get_sample_cov_matrix(X):\n",
        "  \"\"\"\n",
        "  Returns the sample covariance matrix of data X.\n",
        "\n",
        "  Args:\n",
        "    X (numpy array of floats) : Data matrix each column corresponds to a\n",
        "                                different random variable\n",
        "\n",
        "  Returns:\n",
        "    (numpy array of floats)   : Covariance matrix\n",
        "\"\"\"\n",
        "\n",
        "  X = X - np.mean(X, 0)\n",
        "  cov_matrix = 1 / X.shape[0] * np.matmul(X.T, X)\n",
        "  return cov_matrix\n",
        "\n",
        "\n",
        "def sort_evals_descending(evals, evectors):\n",
        "  \"\"\"\n",
        "  Sorts eigenvalues and eigenvectors in decreasing order. Also aligns first two\n",
        "  eigenvectors to be in first two quadrants (if 2D).\n",
        "\n",
        "  Args:\n",
        "    evals (numpy array of floats)    :   Vector of eigenvalues\n",
        "    evectors (numpy array of floats) :   Corresponding matrix of eigenvectors\n",
        "                                         each column corresponds to a different\n",
        "                                         eigenvalue\n",
        "\n",
        "  Returns:\n",
        "    (numpy array of floats)          : Vector of eigenvalues after sorting\n",
        "    (numpy array of floats)          : Matrix of eigenvectors after sorting\n",
        "  \"\"\"\n",
        "\n",
        "  index = np.flip(np.argsort(evals))\n",
        "  evals = evals[index]\n",
        "  evectors = evectors[:, index]\n",
        "  if evals.shape[0] == 2:\n",
        "    if np.arccos(np.matmul(evectors[:, 0],\n",
        "                           1 / np.sqrt(2) * np.array([1, 1]))) > np.pi / 2:\n",
        "      evectors[:, 0] = -evectors[:, 0]\n",
        "    if np.arccos(np.matmul(evectors[:, 1],\n",
        "                           1 / np.sqrt(2)*np.array([-1, 1]))) > np.pi / 2:\n",
        "      evectors[:, 1] = -evectors[:, 1]\n",
        "\n",
        "  return evals, evectors\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_eigenvalues(evals, limit=True):\n",
        "  \"\"\"\n",
        "  Plots eigenvalues.\n",
        "\n",
        "  Args:\n",
        "     (numpy array of floats) : Vector of eigenvalues\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(np.arange(1, len(evals) + 1), evals, 'o-k')\n",
        "  plt.xlabel('Component')\n",
        "  plt.ylabel('Eigenvalue')\n",
        "  plt.title('Scree plot')\n",
        "  if limit:\n",
        "    plt.show()\n",
        "\n",
        "def pca(X):\n",
        "  \"\"\"\n",
        "  Performs PCA on multivariate data. Eigenvalues are sorted in decreasing order\n",
        "\n",
        "  Args:\n",
        "     X (numpy array of floats) :   Data matrix each column corresponds to a\n",
        "                                   different random variable\n",
        "\n",
        "  Returns:\n",
        "    (numpy array of floats)    : Data projected onto the new basis\n",
        "    (numpy array of floats)    : Vector of eigenvalues\n",
        "    (numpy array of floats)    : Corresponding matrix of eigenvectors\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  X = X - np.mean(X, 0)\n",
        "  cov_matrix = get_sample_cov_matrix(X)\n",
        "  evals, evectors = np.linalg.eigh(cov_matrix)\n",
        "  evals, evectors = sort_evals_descending(evals, evectors)\n",
        "  score = change_of_basis(X, evectors)\n",
        "\n",
        "  return score, evectors, evals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZlanf3Pc4hp"
      },
      "source": [
        "#@title Run PCA on voxel or image data\n",
        "#@markdown Note, the image PCA takes about 15mins to run via Colab. \n",
        "#@markdown The voxel PCA is considerably faster (only 3mins).\n",
        "\n",
        "#@markdown This cell also generates a Scree Plot, illustrating the top 50 components.\n",
        "\n",
        "# Load data into data object\n",
        "with np.load(fname) as dobj:\n",
        "    dat = dict(**dobj)\n",
        "\n",
        "## Neural data?\n",
        "X = dat[\"responses\"]\n",
        "\n",
        "## Stimuli?\n",
        "# We need to reshape the images into vectors to accord with MINST analysis code\n",
        "# X = dat[\"stimuli\"]\n",
        "# X = np.reshape(X,[1750,(128*128)])\n",
        "\n",
        "# Visualize\n",
        "# plot_KAY_sample(X)\n",
        "\n",
        "# Perform PCA\n",
        "score, evectors, evals = pca(X)\n",
        "\n",
        "# Plot the eigenvalues\n",
        "plot_eigenvalues(evals, limit=False)\n",
        "plt.xlim([0, 50])  # limit x-axis up to 100 for zooming"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "sLF9w47Ct3Fj"
      },
      "source": [
        "#@title image PCA plotting tools\n",
        "\n",
        "def reconstruct_data(score, evectors, X_mean, K):\n",
        "  \"\"\"\n",
        "  Reconstruct the data based on the top K components.\n",
        "  Args:\n",
        "    score (numpy array of floats)    : Score matrix\n",
        "    evectors (numpy array of floats) : Matrix of eigenvectors\n",
        "    X_mean (numpy array of floats)   : Vector corresponding to data mean\n",
        "    K (scalar)                       : Number of components to include\n",
        "  Returns:\n",
        "    (numpy array of floats)          : Matrix of reconstructed data\n",
        "  \"\"\"\n",
        "\n",
        "  # Reconstruct the data from the score and eigenvectors\n",
        "  # Don't forget to add the mean!!\n",
        "  X_reconstructed =  np.matmul(score[:, :K], evectors[:, :K].T) + X_mean\n",
        "\n",
        "  return X_reconstructed\n",
        "\n",
        "def plot_KAY_reconstruction(X, X_reconstructed_first,X_reconstructed_second):\n",
        "  \"\"\"\n",
        "  Plots 9 images in the MNIST dataset side-by-side with the reconstructed\n",
        "  images.\n",
        "\n",
        "  Args:\n",
        "    X (numpy array of floats)               : Data matrix each column\n",
        "                                              corresponds to a different\n",
        "                                              random variable\n",
        "    X_reconstructed (numpy array of floats) : Data matrix each column\n",
        "                                              corresponds to a different\n",
        "                                              random variable\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "  \"\"\"\n",
        "\n",
        "  plt.figure()\n",
        "  ax = plt.subplot(131)\n",
        "  k = 0\n",
        "  for k1 in range(3):\n",
        "    for k2 in range(3):\n",
        "      k = k + 1\n",
        "      plt.imshow(np.reshape(X[k, :], (128, 128)),\n",
        "                 extent=[(k1 + 1) * 128, k1 * 128, (k2 + 1) * 128, k2 * 128])\n",
        "  plt.xlim((3 * 128, 0))\n",
        "  plt.ylim((3 * 128, 0))\n",
        "  plt.tick_params(axis='both', which='both', bottom=False, top=False,\n",
        "                  labelbottom=False)\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "  plt.title('Data')\n",
        "  ax = plt.subplot(132)\n",
        "  k = 0\n",
        "  for k1 in range(3):\n",
        "    for k2 in range(3):\n",
        "      k = k + 1\n",
        "      plt.imshow(np.reshape(np.real(X_reconstructed_first[k, :]), (128, 128)),\n",
        "                 extent=[(k1 + 1) * 128, k1 * 128, (k2 + 1) * 128, k2 * 128])\n",
        "  plt.xlim((3 * 128, 0))\n",
        "  plt.ylim((3 * 128, 0))\n",
        "  plt.tick_params(axis='both', which='both', bottom=False, top=False,\n",
        "                  labelbottom=False)\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "  plt.title('1st component')\n",
        "  plt.tight_layout()\n",
        "  ax = plt.subplot(133)\n",
        "  k = 0\n",
        "  for k1 in range(3):\n",
        "    for k2 in range(3):\n",
        "      k = k + 1\n",
        "      plt.imshow(np.reshape(np.real(X_reconstructed_second[k, :]), (128, 128)),\n",
        "                 extent=[(k1 + 1) * 128, k1 * 128, (k2 + 1) * 128, k2 * 128])\n",
        "  plt.xlim((3 * 128, 0))\n",
        "  plt.ylim((3 * 128, 0))\n",
        "  plt.tick_params(axis='both', which='both', bottom=False, top=False,\n",
        "                  labelbottom=False)\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "  plt.title('15th component')\n",
        "  plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hStvKQl_r28E"
      },
      "source": [
        "K = 1\n",
        "\n",
        "# Reconstruct the data based on all components\n",
        "X_mean = np.mean(X, 0)\n",
        "X_reconstructed = reconstruct_data(score, evectors, X_mean, K)\n",
        "\n",
        "K = 15\n",
        "\n",
        "# Reconstruct the data based on all components\n",
        "X_mean = np.mean(X, 0)\n",
        "X_reconstructed_2 = reconstruct_data(score, evectors, X_mean, K)\n",
        "\n",
        "# Plot the data and reconstruction\n",
        "plot_KAY_reconstruction(X, X_reconstructed,X_reconstructed_2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygeICj6U1fo3"
      },
      "source": [
        "Now that we have the principles components we can start to build a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enp86CGHmNva"
      },
      "source": [
        "\n",
        "\n",
        "# LDA\n",
        "\n",
        "# From labels to voxel space, expands dimensionality so much you'll run into problems\n",
        "\n",
        "\n",
        "\n",
        "# Train on voxels to predict the images\n",
        "\n",
        "# PCA on the voxels\n",
        "# Train LDA on first X principles components to predict the labels\n",
        "# Cut out some of the weights\n",
        "# Try evaluation in reverse\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LF090E7lcym"
      },
      "source": [
        "#Logistic Regression approach\n",
        "The next set of cells build a logistic regression that predicts the identity of an image (`animal` or `not animal`) based upon the neural data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPeBUvcSTedV",
        "cellView": "form"
      },
      "source": [
        "#@title Create training and test vectors\n",
        "#@markdown This builds vectors of binarised data categorised as `animal` or `not animal`.\n",
        "#@markdown We print a few cells to ensure it has worked.\n",
        "\n",
        "# Load image labels for training images\n",
        "labels = np.load('kay_labels.npy')\n",
        "\n",
        "# Check labels at highest level of abstraction\n",
        "find_animals = labels[0,:]\n",
        "print(np.unique(find_animals))\n",
        "print(find_animals[0:7])\n",
        "\n",
        "# Find labels and build binarised list of animals and not animals\n",
        "just_animals = np.array(range(len(find_animals)))\n",
        "for i in range(len(find_animals)):\n",
        "  if find_animals[i] == 'animal':\n",
        "    just_animals[i] = 1\n",
        "  else:\n",
        "    just_animals[i] = 0\n",
        "\n",
        "print(just_animals[0:7]) # OK\n",
        "\n",
        "# Load image labels for validation images\n",
        "val_labels = np.load('kay_labels_val.npy')\n",
        "\n",
        "# Check labels at highest level of abstraction\n",
        "find_animals = val_labels[0,:]\n",
        "print(find_animals[0:7])\n",
        "\n",
        "# Find labels and build binarised list of animals and not animals\n",
        "test_animals = np.array(range(len(find_animals)))\n",
        "for i in range(len(find_animals)):\n",
        "  if find_animals[i] == 'animal':\n",
        "    test_animals[i] = 1\n",
        "  else:\n",
        "    test_animals[i] = 0\n",
        "\n",
        "print(test_animals[0:7]) # OK\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlqHJ02NYt_P"
      },
      "source": [
        "We have successfully converted the training and test data into a training and test vector of binarised values where ratings of 1 are `animal` and ratings of 0 are `not animal`. \n",
        "\n",
        "Next, we pass the training vector to `sklearn.LogisticRegression` to create our model that attempts to predict the animal label from voxel activity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MWCXU-f_YD1"
      },
      "source": [
        "# Let's get some insight in the Kay data set with a view to decode it\n",
        "with np.load(fname) as dobj:\n",
        "    dat = dict(**dobj)\n",
        "\n",
        "#print(dat[\"stimuli\"].shape) # (1750,128,128) energy per pixel\n",
        "#print(dat[\"responses\"].shape) # (1750,8428) voxel activity per image \n",
        "\n",
        "# Can we predict stimulus as a function of voxel activity\n",
        "# y = dat[\"stimuli\"]\n",
        "y = just_animals \n",
        "X = dat[\"responses\"]\n",
        "\n",
        "# # We need to reduce dimensionality of stimuli\n",
        "# # This is a toy example, reducing to binarised activity for a single pixel\n",
        "# y = y[:,64,64]\n",
        "\n",
        "# # Convert to binary data (if statements don't work for whatever reason)\n",
        "# mask = y > 0\n",
        "# y[mask,] = 1\n",
        "# mask = y <= 0\n",
        "# y[mask,] = 0\n",
        "\n",
        "# plt.hist(y,bins = 300)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Includes L2 \"ridge\" regularisation to reduce overfitting \n",
        "# L1 \"sparseness\" regularisation makes more sense but it took too long to run  \n",
        "log_reg = LogisticRegression(penalty=\"l2\", C=1, max_iter = 2000).fit(X,y)\n",
        "\n",
        "# X_test = dat[\"responses_test\"]\n",
        "# y_pred = log_reg.predict(X)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mofaILLoapK-"
      },
      "source": [
        "We have now created the model (`log_reg`). To test the model we can crossvalidate and/or examine its accuracy using the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLbUfR0aLc0U"
      },
      "source": [
        "# Let's test accuracy using the test dataset\n",
        "\n",
        "X_test = dat[\"responses_test\"] # Here's the neural response during test\n",
        "y_test = test_animals\n",
        "\n",
        "# # We need to subset and binarise the stimuli data like we did for the training set\n",
        "# y_test = y_test[:,64,64]\n",
        "\n",
        "# # Convert to binary data (if statements don't work for whatever reason)\n",
        "# mask = y_test > 0\n",
        "# y_test[mask,] = 1\n",
        "# mask = y_test <= 0\n",
        "# y_test[mask,] = 0\n",
        "\n",
        "# Compute train accuracy\n",
        "test_accuracy = compute_accuracy(X_test, y_test, log_reg)\n",
        "print(f\"Accuracy on the test data: {test_accuracy:.2%}\")\n",
        "\n",
        "# Test accuracy is 61.67%\n",
        "# Cross validation accuracy is 70.52%\n",
        "\n",
        "accuracies = cross_val_score(log_reg, X, y, cv=8) # k=8 crossvalidation\n",
        "\n",
        "plot_accuracy(accuracies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARmcbJJibFbE"
      },
      "source": [
        "Accuracy is OK although the cross-validation indicates we are overfitting (we have many more features than samples). We need to complete some form of dimension reduction on the neural data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yURXl635c1eV"
      },
      "source": [
        "# Time to do some PCA\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe2l4MKGc4JT"
      },
      "source": [
        "## Neural Net approach\n",
        "An alternative method to understand the Kay dataset is using a convolutional neural network to predict neural activity from images. \n",
        "\n",
        "This is a rough approximation of human visual perception so by reducing the type of information available in the image we may be able to simulate the type of symptoms that characterise semantic dementia.\n",
        "\n",
        "Look at the approach by ...author... recommended by Philipp."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M4pTZ11deZ2"
      },
      "source": [
        "# Here be CNNs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpwckf2YduOZ",
        "cellView": "form"
      },
      "source": [
        "#@title Playing with data\n",
        "this = np.unique(labels[2,:])\n",
        "print(this.shape)\n",
        "# print(this)\n",
        "\n",
        "that = np.unique(labels[3,:])\n",
        "print(that.shape)\n",
        "# print(that)\n",
        "\n",
        "# dict(zip(np.unique(labels[0,:]), np.bincount(labels[0,:])))\n",
        "\n",
        "# from collections import Counter\n",
        "\n",
        "# this = Counter(labels[0,:])\n",
        "\n",
        "# print(type(labels))\n",
        "\n",
        "# this = np.unique(labels[0,:], return_counts=True)\n",
        "\n",
        "# fig, ax = plt.subplots()    \n",
        "# fig.set_size_inches(9, 5)\n",
        "# ax.barh(y = this[0], width = this[1])\n",
        "# plt.title(\"Labels at Level #1\")\n",
        "# plt.xlabel(\"Counts\")\n",
        "# for i, v in enumerate(this[1]):\n",
        "#     ax.text(v+5, i-0.1, str(v), color='black')\n",
        "\n",
        "# this = np.unique(labels[1,:], return_counts=True)\n",
        "\n",
        "# fig, ax = plt.subplots() \n",
        "# fig.set_size_inches(9, 12)   \n",
        "# ax.barh(y = this[0], width = this[1])\n",
        "# plt.title(\"Labels at Level #2\")\n",
        "# plt.xlabel(\"Counts\")\n",
        "# for i, v in enumerate(this[1]):\n",
        "#     ax.text(v+5, i-0.1, str(v), color='black')\n",
        "\n",
        "\n",
        "this = labels[3,labels[2,:]==\"mammal\"]\n",
        "\n",
        "this = np.unique(this, return_counts=True)\n",
        "\n",
        "fig, ax = plt.subplots() \n",
        "fig.set_size_inches(9, 12)   \n",
        "ax.barh(y = this[0], width = this[1])\n",
        "plt.title(\"Categories within vertebrate animals\")\n",
        "plt.xlabel(\"Counts\")\n",
        "for i, v in enumerate(this[1]):\n",
        "    ax.text(v+5, i-0.1, str(v), color='black')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etSxoTmGLtEk"
      },
      "source": [
        "#@title LDA\n",
        "\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}